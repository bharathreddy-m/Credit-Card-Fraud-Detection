# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B1hLrxDt6WSd11ltx0ioTxtlxq00AjC4

# Credit card fraud detection - classification
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/creditcard.csv.zip')

df

df.shape

df['Class'].value_counts()

# see all data types
df.info()

# summary of dataset
df.describe()

# see null values in dataset
df.isnull().sum()

# see null values
null = df.isna().sum()
print(null)

# check for duplicates
duplicates = df.duplicated().sum()
print(duplicates)

# drop all duplicates
df.drop_duplicates(inplace=True)

# see dataset shape
df.shape

df['Class'].value_counts()

"""Perform class imbalances - undersample class-0, oversmaple class-1"""

a = df.drop('Class', axis=1)
a.shape

b = df['Class']
b.shape

# Do oversample and undersample
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Initialize oversampler and undersampler
oversampler = RandomOverSampler(sampling_strategy={1: 141627}, random_state=42)
undersampler = RandomUnderSampler(sampling_strategy={0: 165122}, random_state=42)

# First, oversample class 1
x_over, y_over = oversampler.fit_resample(a, b)

# Then, undersample class 0
x_balanced, y_balanced = undersampler.fit_resample(x_over, y_over)

# Combine features and target variable back into a DataFrame
df_balanced = pd.DataFrame(x_balanced, columns=a.columns)
df_balanced['Class'] = y_balanced

# Check the class distribution
class_distribution = df_balanced['Class'].value_counts()
print("Class distribution after balancing:")
print(class_distribution)

# Save the balanced dataset if needed
# df_balanced.to_csv('balanced_dataset.csv', index=False)  # Uncomment to save

df_balanced['Class'].value_counts()

"""Checking for skewness in the data and outliers"""

# see outliers in data
# Method 1: Interquartile Range (IQR)
import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df_balanced.quantile(0.25)
Q3 = df_balanced.quantile(0.75)

# Calculate IQR
IQR = Q3 - Q1

# Define outlier condition
outlier_condition = (df_balanced < (Q1 - 1.5 * IQR)) | (df_balanced > (Q3 + 1.5 * IQR))

# Count the number of outliers in each numerical column
outliers_count = outlier_condition.sum()
outliers_count

# Calculate skewness for each feature
skewness = df_balanced.drop('Class', axis=1).skew()
print("Skewness of each feature:\n", skewness)

# Optionally, visualize the skewness
plt.figure(figsize=(10, 6))
skewness.plot(kind='bar')
plt.title('Skewness of Features')
plt.xlabel('Features')
plt.ylabel('Skewness')
plt.show()

"""Applying transformation to reeduce outliers and skewness in data"""

from sklearn.preprocessing import PowerTransformer

# Select only numerical features (excluding 'Class')
numerical_features = df_balanced.drop(columns=['Class']).columns

# Apply Yeo-Johnson transformation
pt = PowerTransformer(method='yeo-johnson')
df_balanced[numerical_features] = pt.fit_transform(df_balanced[numerical_features])

# Verify Class remains unchanged
df_balanced['Class'] = df_balanced['Class'].astype(int)

# Check skewness again
print(df_balanced.skew())

print(df_balanced['Class'].unique())  # Should be only 0 and 1

print(df_balanced['Class'].dtype)

df_balanced['Class'] = df_balanced['Class'].astype(int)

print(df_balanced['Class'].value_counts(normalize=True))

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.boxplot(data=df_balanced)  # replace with your relevant columns
plt.title('Boxplot of Features to Detect Outliers')
plt.show()

#cheking skewness of the data
df_balanced.skew()

# Histogram
import matplotlib.pyplot as plt
import seaborn as sns

# Plot histograms for numerical columns
df_balanced.hist(figsize=(12, 8), bins=30, edgecolor='black')
plt.suptitle("Histograms of Numerical Columns", fontsize=14)
plt.show()

# Skewness & Kurtosis - Numerical measures like skewness and kurtosis give an indication of whether the data follows a normal distribution.
df_skew_kurt = df_balanced.agg(['skew', 'kurtosis']).T
print(df_skew_kurt)



"""Feature scaling"""

l = df_balanced.drop('Class', axis=1)
l.columns
l.shape

m = df_balanced['Class']
m.shape

# feature scaling - standardscaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

l_scaled = scaler.fit_transform(l)

df_scaler = pd.DataFrame(l_scaled, columns=l.columns)

df_scaler['Class'] = m.values

df_scaler

df_scaler['Class'].value_counts()

"""If you want to use 50% of the data"""

df_sampled = df_scaler.sample(frac=0.5, random_state=42)

df_sampled.shape

x = df_sampled.drop('Class', axis=1)
x.shape

y = df_sampled['Class']
y.shape

"""MODEL"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

#fitting all models at once
a = LinearRegression()
b = GaussianNB()
c = KNeighborsClassifier(n_neighbors=8)
d = SVC()
e = DecisionTreeClassifier()
f = RandomForestClassifier(n_estimators=200)

#Fit all models at once and see their accuracies
models = [a,b,c,d,e,f]
model_names = ['Logistic regression', 'gaussian nb', 'knn', 'svc', 'decision tree', 'random forest']

import seaborn as sns
import matplotlib.pyplot as plt

# Dictionary to store results
results = {}

# Run all models at once
for model, names in zip(models, model_names):
  #Fit the model
  model.fit(x_train, y_train)

  #Make predictions
  training_error = model.predict(x_train)
  testing_error = model.predict(x_test)

  #convert y_pred into binary not continuous
  training_error = (training_error>0.5).astype(int)
  testing_error = (testing_error>0.5).astype(int)

  #Calculate Training error
  train_accuracy = accuracy_score(y_train, training_error)
  train_cm = confusion_matrix(y_train, training_error)
  train_cr = classification_report(y_train, training_error)
  #Visualize confusion metrics
  #sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues')
  #plt.title("Training Confusion Metrics")
  #plt.show()

  #Calculate Test error
  test_accuracy = accuracy_score(y_test, testing_error)
  test_cm = confusion_matrix(y_test, testing_error)
  test_cr = classification_report(y_test, testing_error)
  #visualize confusion metrics
  #sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues')
  #plt.title("Testing Confusion Metrics")
  #plt.show()

  # Store results correctly
  results[names] = (train_accuracy, test_accuracy)
  print(f"✅ Model {names} added to results.\n")  # Debugging line

  #print results
  print(f'Model name -- {names}')
  print(f'Training results -- Accuracy={train_accuracy}')
  print(f'Confusion metrics=\n{train_cm}')
  print(f'Classification report=\n{train_cr}')
  print("-" * 100)
  print(f'Testing results -- Accuracy={test_accuracy}')
  print(f'Confusion metrics=\n{test_cm}')
  print(f'Classification report=\n{test_cr}')
  print("||" * 100)

# Display all results in one comparison table
print("\nComparison of All Models' Accuracy:\n")
print(f"{'Model Name':<25} {'Train Accuracy':<20} {'Test Accuracy':<20}")
print("=" * 70)

# Check if results have been stored
if not results:
    print("⚠️ No models found! Check if models are training correctly.")

for name, (train_acc, test_acc) in results.items():
    print(f"{name:<25} {train_acc * 100:.2f}%{'':<10} {test_acc * 100:.2f}%")

